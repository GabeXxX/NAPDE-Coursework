{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9445466b-46ea-494a-aba8-9954aee43568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 10:40:35.620762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import scipy.sparse\n",
    "import scipy.linalg\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554018aa-3660-4150-854b-e74b103d7828",
   "metadata": {},
   "source": [
    "# Inverse Problem\n",
    "Given parameters of interest (PoI) and observation of a state variable, consider the folowing additive noise model\n",
    "$$\n",
    "\\textbf{y} = F(\\textbf{u}) + \\textbf{e}\n",
    "$$\n",
    "where $\\textbf{u}$ are the PoI, $F$ is the parameter-to-observable(Pto) map and $\\textbf{y}$ represent observational data.  \n",
    "\n",
    "__Goal:__ determine the parameter of interest $\\textbf{u}$ given the observational data $\\textbf{y}$.  \n",
    "\n",
    "Solving the inverse problem usually involves solving an optimization problem of the form\n",
    "$$\n",
    "\\min_{\\textbf{u}} \\lVert \\textbf{y} - F(\\textbf{u}) \\rVert_{2}^2 + R(\\textbf{u})\n",
    "$$\n",
    "where $ R(\\textbf{u})$ is a regularization term to reduce the size of the solution space, since the problem is usually ill-posed (many possible solution exist that are coherent with our data).  \n",
    "\n",
    "__Problem:__ the optimization of such functional is computationally expansive.  \n",
    "\n",
    "__Solution:__ learning a data-driven solver that after an offline training stage (expansive but done only one time), output estimate of our PoI.  \n",
    "\n",
    "Given as training set  $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$, if we use a neural network $\\Psi$ then our solver is paramerized by the weight of the network $\\textbf{W}$. This solver require the optimization of the following functional\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\lVert \\textbf{u}^{(m)} - \\Psi(\\textbf{y}^{(m)}, \\textbf{W}) \\rVert_{2}^2 + R(\\textbf{W})\n",
    "$$\n",
    "\n",
    "Instead of regularize the weight of the network directly, we can regularize the output of the network, informing the optimization procedure of the inversion task, of the properties of the noise afflicting our observational data and the knowledge about some physical properties of the PoI we posses. This lead to the following optimization objective\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\lVert \\textbf{u}^{(m)} - \\Psi(\\textbf{y}^{(m)}, \\textbf{W}) \\rVert_{2}^2 + \\lVert M (\\textbf{y} - F(\\Psi(\\textbf{y}^{(m)}, \\textbf{W}))) \\rVert_{2}^2 + \\lVert P (\\Psi(\\textbf{y}^{(m)}, \\textbf{W})) \\rVert_{2}^2\n",
    "$$\n",
    "where $M,P$ are some mapping representing information about noise and PoI respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32247cb6-e1eb-4a8e-b6fc-c32b80fe753f",
   "metadata": {},
   "source": [
    "# Bayesian Inverse Problem\n",
    "By adopting a probabilisitc framework instead of deterministic, the question asked by the inverse problem essentially changes from “what is the value of our parameter?” to “how accurate is the estimate of our parameter?”.   \n",
    "\n",
    "In this setting, inverse problem deal with the following observational model\n",
    "$$\n",
    "    Y = F(U) + E\n",
    "$$\n",
    "where $F$ is __Parameter-to-Observable(PtO)__ map and $Y,U,E$ are random variable representing respectively the observational data, the __Parameters of Intereste(PoI)__ and the noise model.  \n",
    "\n",
    " \n",
    "\n",
    "__Goal:__ model the posterior distribution $P(\\textbf{u} | \\textbf{y})$, i.e \"given the observational data, what is the distribution of the parameters of interest?\"\n",
    "\n",
    "Using Bayes' Theorem\n",
    "$$\n",
    "P(\\textbf{u} | \\textbf{y}) \\propto P(\\textbf{y} | \\textbf{u})P(\\textbf{u})\n",
    "$$\n",
    "The assumptions usually made are:\n",
    "- $E \\sim N(\\mu_E, \\Gamma_E)$, $U \\sim N(\\mu_{pr}, \\Gamma_{pr})$, $E\\perp U$\n",
    "- $ P(\\textbf{y} | \\textbf{u}) = P_E (\\textbf{y} - F(\\textbf{u}))$\n",
    "\n",
    "\n",
    "__Problem:__ computing such probability is often intractable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc4beb-a7dc-4f81-be68-d374156561ad",
   "metadata": {},
   "source": [
    "__Solution:__ using variational inference to approximate such distributions.\n",
    "\n",
    "Denoting by  $P(\\textbf{u} | \\textbf{y})$ the target posterior density we want to approximate, __variational inference__ perform such approximation choosing a set of probability distributin $Q_{\\phi}(\\textbf{u} | \\textbf{y})$ parameterized by a parameter $\\phi$ and then finiding the distribution in such family \"closest\" to the target one.  \n",
    "\n",
    "To do that some notion of distance between $Q_{\\phi}(\\textbf{u} | \\textbf{y})$ and $P(\\textbf{u} | \\textbf{y})$ must be introduced.  \n",
    "\n",
    "Normally the Kullback-Leibler divergence (KLD) is used to obtain the evidence lower bound (ELBO), an useful lower bound on the log-likelihood of some observed data, and the goal of variational inference is to minimize the KL divergence, or, equivalently, maximizing the evidence lower bound (ELBO).\n",
    "\n",
    "Following the work in the reference [1], instead of KLD, a family of Jensen-Shannon Divergence (JSD) is used, leading to the minimization of the following quantity\n",
    "$$\n",
    "\\frac{1-\\alpha}{\\alpha} KL(P(\\textbf{u} | \\textbf{y}) | Q_{\\phi}\\textbf{u} | \\textbf{y})) - \\mathbb{E}_{\\textbf{u} \\sim Q_{\\phi}} \\left[ \\log(P(\\textbf{y} | \\textbf{u})) \\right] + KL(Q_{\\phi}(\\textbf{u} | \\textbf{y}) |P(\\textbf{u}))\n",
    "$$\n",
    "where $KL(\\cdot,\\cdot)$ denote the Kullback-Leibler divergence.  \n",
    "\n",
    "__Objective:__  incorporate such minimization problem into a deep learning framework, i.e recave a differentiable loss function in the form of the one written above in the deterministic setting to train a neural network to perform the variational inference task. \n",
    "\n",
    "To do so:\n",
    "- consider a training set  $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$\n",
    "- apply the fact that the minimization of the KLD between the empirical and model distributions is equivalent to maximization of the likelihood function with respect to $\\phi$.\n",
    "- form a Monte-Carlo estimation using our PoI data\n",
    "- adopt a Gaussian model for our model posterior $Q_{\\phi}(\\textbf{u} | \\textbf{y}^{(m)})= N(\\textbf{u} | \\mu_{post}^{(m)}, \\Gamma_{post}^{(m)})$\n",
    "- adopt the Gaussian prior distribution for the noise and the PoI model as assumed above\n",
    "\n",
    "Detail are in the reference [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc77a29-a907-4191-82bc-f358209a85f9",
   "metadata": {},
   "source": [
    "Considering a neural network $\\Psi$ with paramters $\\textbf{W}$ that takes as input our observation data $\\textbf{y}^{(m)}$ and output the statistics of our approximated posterior model $\\phi^{(m)} = (\\mu_{post}^{(m)}, \\Gamma_{post}^{(m)})$, the following loss function is obtained:\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\frac{1-\\alpha}{\\alpha} (\\log |\\Gamma_{post}^{(m)}| + \\lVert \\mu_{post}^{(m)}- \\textbf{u}^{(m)}) \\rVert_{\\Gamma_{post}^{(m)-1}}^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "    + \\lVert \\textbf{y}^{(m)} - F(\\textbf{u}_{draw}^{(m)}(\\textbf{W})) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "     + \\text{tr}(\\Gamma_{pr}^{-1}\\Gamma_{post}^{(m)}) +   \\lVert \\mu_{post}^{(m)}- \\textbf{u}_{(pr)}) \\rVert_{\\Gamma_{pr}^{-1}}^2 + \\log \\frac{\\Gamma_{pr}}{\\Gamma_{post}^{(m)}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    (\\mu_{post}^{(m)}, \\Gamma_{post}^{\\frac{1}{2}(m)}) = \\Psi (\\textbf{y}^{(m)}, \\textbf{W})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\textbf{u}_{draw}^{(m)}(\\textbf{W}) = \\mu_{post}^{(m)} + \\Gamma_{post}^{\\frac{1}{2}(m)} \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\epsilon \\sim N(0, \\textbf{I}_D)\n",
    "$$\n",
    "\n",
    "During the training procedure, the repeated operation of the PtO map on our dataset $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$ may incur a significant computational cost. To alleviate this the PtO map is replaced with another neural network $\\Psi_d$ parameterized by weights $\\textbf{W}_d$. This is reflected in the following modification of the loss function\n",
    "\n",
    "\n",
    "$$\n",
    "     \\lVert \\textbf{y}^{(m)} - F(\\textbf{u}_{draw}^{(m)}(\\textbf{W})) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2  \\rightarrow \\lVert \\textbf{y}^{(m)} - \\Psi_d(\\textbf{u}_{draw}^{(m)}(\\textbf{W}), \\textbf{W}_d) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2\n",
    "$$\n",
    "\n",
    "As exaplained in the paragraph below, our loss function is regularized by the likelihood model containing the PtO map and the prior model containing information on our PoI space.\n",
    "![](./assets/UQ-VAE.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74be2f-fb12-42ee-8eff-2bda5afd2559",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "## Poisson Problem\n",
    "Consider the following modification of Diriclet problem for the poisson equation in two dimension, $\\textbf{x} = (x,y) \\in \\Omega \\subset R^2$, where we add a conductivity term $q(\\textbf{x})$\n",
    "$$\n",
    "\\begin{cases}\n",
    "    - q(\\textbf{x})\\Delta u(\\textbf{x}) = f(\\textbf{x}) \\;\\;\\;\\;\\;\\;\\;\\;\\;  \\textbf{x} \\in \\Omega \\\\\n",
    "    u(\\textbf{x}) = 0 \\;\\;\\;\\;\\;\\;\\;\\;\\;  \\textbf{x} \\in \\partial \\Omega\n",
    "\\end{cases}\n",
    "$$\n",
    "where $q(\\textbf{x})$ is the conductivity coefficent, our parameter of interest, and $\\Omega = (0,1)^2$.\n",
    "\n",
    "## Prior model\n",
    "TODO\n",
    "\n",
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa36dbf-e28a-4076-9ee7-07f0296dfca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb4e511-a8ba-4d70-b99b-5d02c8e90560",
   "metadata": {},
   "source": [
    "# Neural Netork architecture and training\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea68a5-f8e0-404c-a9c5-f560599f16c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cf1ec6-bb10-420d-b5a2-1f29f74da36a",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Solving Bayesian Inverse Problems via Variational Autoencoders, Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh, \n",
    "https://doi.org/10.48550/arXiv.1912.04212"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-env",
   "language": "python",
   "name": "bayesian-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
